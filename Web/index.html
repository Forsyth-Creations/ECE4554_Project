<!-- Write me a basic, nice looking webpage -->
<!-- It should contain at least 3 different paragraphs and a header -->

<!DOCTYPE html>
<html>

<head>
    <title>Web</title>
    <!-- Import CSS -->
    <link rel="stylesheet" type="text/css" href="style.css">
</head>

<body>
    <t>Rock, Paper, Scissors: YOLO and Roboflow </t>
    <p class="caption">By: Henry Forsyth, Shefali Ranjan, Sagar Ranga</p>

    <div class="background2 flex-row">
        <div>
            <h1>Abstract:</h1>
            <p>In this project, we wanted to develop a real-time hand gesture recognition system that leverages a
                pre-trained model to accurately detect and classify hand gestures for playing rock-paper-scissors
                against a
                user. We use an Ultralytics YOLOv8 model with the PyTorch deep learning framework for this task. The
                model
                is trained on a custom dataset for rock-paper-scissors gestures. The confidence level is pulled from the
                model along with a bounding box to display the model prediction live. Due to local machine limitations,
                the
                model is trained on various subsets of our data and improved incrementally. The final result is a robust
                model that can accurately classify most cases of rock, paper and scissors.</p>
        </div>
        <!-- Import an image -->
        <img src="public/teaser.png" alt="teaser" class="basicImage">
    </div>

    <div class="background1">
        <h1>Introduction:</h1>
        <p>In the ever evolving landscape of human-computer interaction, the development of artificial intelligence
            (AI) models to recognize and interpret intricate hand gestures has become an exciting frontier. Our
            motivation was to integrate real-world actions with digital software. Specifically, our goal was to
            develop a real-time hand gesture recognition system to accurately detect and classify hand gestures for
            playing rock-paper-scissors against a user.
        </p>
        <p>
            The application of real-time hand gesture recognition models is huge considering that industry leaders
            like Apple and Meta have invested large swaths of capital to develop their products like Apple Vision
            Pro and Meta Quest. Their products actively use headset cameras for tracking hand movements and
            detecting hand gestures. Without human-computer interaction devices, hand gestures become the new method
            of interacting with their headsets to perform various tasks.
        </p>
        <p>
            Existing methodologies for hand gesture recognition have improved quite a lot. Prior methodologies often
            dealt with limitations in accuracy and real-time responsiveness. As a result, this project uses a highly
            popular model known as YOLO. It uses one of the best neural network architectures to produce high
            accuracy and overall processing speed, which perfectly matches with our requirements.
        </p>
    </div>



    <div class="background2">
        <h1>Approach: </h1>
        <p>
            To create a model that could be able to detect three different hand positions (rock, paper, and scissors),
            we first had to create a dataset and annotate each model with a bounding box around the item we wished to
            classify. We found a great base dataset from Roboflow (public dataset: Rock Paper Scissors Dataset
            raw-300x300). To add more variety to the dataset, in terms of skin coloration, background colors, and
            direction of the image, we decided to take the original dataset and add more images of our own hands to add
            more data points. However, since the original dataset had not been annotated, we opted for picking a small
            subsample of images first to iteratively test the model before proceeding. Also, applied transformations to
            the images using built-in Roboflow helper functions, skewed some images, and changed the saturation of some
            images to add different data points. After generating more data points that were able to add more variety,
            we used Roboflow to annotate each image for our YOLOv8 model.
        </p>

        <p>
            We started with a pre-trained YOLOv8 model (pre-trained weights, yolov8n.pt). We started training with our
            custom dataset, through evaluation on a diverse set of images. Whenever we realized that there was a certain
            area that our training set was missing we added more images to the training set to result in the more
            inclusive model we could create. Through iteration, we were able to achieve a robust model that performed
            optimally. To visualize our results during evaluation, we used the OpenCV Library. This Library allowed us
            to observe our model’s performance in different scenarios and enabled on-the-fly adjustments for improved
            accuracy.
        </p>
    </div>

    <div class="background1">
        <h1>
            Experiments and Results:
        </h1>
        <p>
            At the onset of this assignment, we were interested in producing a CV model that can determine if a rock,
            paper, or scissors hand is played in the classic game of “rock, paper, scissors”. For our baseline, we
            considered that the game is played with a strategy that results in the person having a 33.3% chance of
            guessing the correct counter-hand. We considered this to be the naive model for the game: any detections
            with confidence levels higher than this default were considered good data. However, we noted that a CV model
            with accuracy and confidence of 40% would be ineffective, inaccurate, and unusable. As such, as we
            experimented with creating a model, we sought as high an accuracy as possible (the value threshold we will
            discuss later).
        </p>
        <p>
            Understanding that we were generating a classifier, we could have implemented basic feature tracking based
            on equivalent changes in the gradient of the image. We could then use those features and manually classify
            them, and compare the data points directly to create an appropriate model. This approach, while plausible,
            would have more than doubled the time to market. Instead, we favored training our own CNN to classify the
            elements. To accomplish this, we leveraged the following:
        </p>
        <p>
            <b>RoboFlow:</b>
        <p>
            An online website that allows you to import massive datasets and annotate the images with
            either a
            classification, segmentation, or others of the like. RoboFlow has built-in training protocols, but for our
            purposes we favored training locally with YOLOv8
        </p>
        </p>
        <p>
            <b>YOLOv8 and Cuda run locally</b>.
        <p>
            This allowed us to leverage local development, as well as avoid the
            costs of
            training models on RoboFlow (which occurs after training more than three models). All training was
            accomplished on an NVIDIA 2070, using Cuda
        </p>
        </p>
    </div>

</body>

</html>